# 分词

## 为什么中文分词

* 词是最小的能够独立活动的有意义的语言成分
* 汉语是以字位单位，不像西方语言，词与词之间没有空格之类的标志指示词的边界
* 分词问题为中文文本处理的基础性工作,分词的好坏对后面的中文信息处理其关键作用

## 中文分词的难点

* 分词规范，词的定义还不明确 (《统计自然语言处理》宗成庆)
* 歧义切分问题，交集型切分问题，多义组合型切分歧义等
  - 结婚的和尚未结婚的 =>
  - 结婚／的／和／尚未／结婚／的
  - 结婚／的／和尚／未／结婚／的
* 未登录词问题
  * 已有的词表中没有收录的词
  * 已有的训练语料中未曾出现过的词，第二种含义中未登录词又称OOV(Out of Vocabulary)。对于大规模真实文本来说，未登录词对于分词的精度的影响远超歧义切分。一些网络新词，自造词一般都属于这些词。

## 汉语分词方法

* 基于字典、词库匹配的分词方法(基于规则)
  - 基于字符串匹配分词，机械分词算法
  - 将待分的字符串与一个充分大的机器词典中的词条进行匹配
  - 分为正向匹配和逆向匹配；最大长度匹配和最小长度匹配
  - 单纯分词和分词与标注过程相结合的一体化方法
  - 所以常用的有：正向最大匹配，逆向最大匹配，最少切分法。实际应用中，将机械分词作为初分手段，利用语言信息提高切分准确率
  - 优先识别具有明显特征的词，以这些词为断点，将原字符串分为较小字符串再机械匹配，以减少匹配错误率，或将分词与词类标注结合
  - 从一个完整的句子里，按照从左向右的顺序，识别出多种不同的3个词的组合，然后根据下面的4条消歧规则，确定最佳的备选词组合
  - 选择备选词组合中的第1个词，作为1次迭代的分词结果；剩余的词(即这个句子中除了第一个已经分出的词的剩余部分)继续进行下一轮的分词运算。采用这种办法的好处是，为传统的前向最大匹配算法加入了上下文信息，解决了其每次选词只考虑词本身，而忽视上下文相关词的问题
    + Simple方法，即简单的正向匹配，根据开头的字，列出所有可能的结果。比如“国际化大都市”，可以得到： 国 国际 国际化
    + Complex方法，匹配出所有的“三个词的词组”（即原文中的chunk，“词组”），即从某一既定的字为起始位置，得到所有可能的“以三个词为一组”的所有组合，比如“研究生命起源”，可以得到 研_究_生 研_究_生命 研究生_命_起源 研究_生命_起源
  - 消除歧义的规则”有四个，使用中依次用这四个规则进行过滤，直到只有一种结果或者第四个规则使用完毕，4条消歧规则包括：
    + 备选词组合的长度之和最大（Maximum matching (Chen & Liu 1992)）
      * 对“simple”匹配方法，选择长度最大的词，用在上文的例子中即选择“国际化”；
      * 对“complex”匹配方法，选择“词组长度最大的”那个词组，然后选择这个词组的第一个词，作为切分出的第一个词，如上文 的例子中即“研究生_命_起源”中的“研究生”，或者“研究_生命_起源”中的“研究”
    + 备选词组合的平均词长最大（Largest average word length (Chen & Liu, 1992)）
      * 比如“生活水平”，可能得到如下词组： 生_活水_平 (4/3=1.33) 生活_水_平 (4/3=1.33) 生活_水平 (4/2=2) 根据此规则，就可以确定选择“生活_水平”这个词组。
    + 备选词组合的词长变化最小（Smallest variance of word lengths (Chen & Liu, 1992)）
      * 研究_生命_起源 （标准差=sqrt(((2-2)^2+(2-2)^2+(2-2^2))/3)=0）
      * 研究生_命_起源 （标准差=sqrt(((2-3)^2+(2-1)^2+(2-2)^2)/3)=0.8165）
    + 备选词组合中，单字词的出现频率统计值最高(取单字词词频的自然对数，然后将得到的值相加，取总和最大的词)（Largest sum of degree of morphemic freedom of one-character words）
      * “设施和服务”，这个会有如下几种组合 设施_和服_务_设施_和_服务_ 设_施_和服_
      * 经过规则1过滤得到： 设施_和服_务_设施_和_服务_
      * 规则2和规则3都无法得到唯一结果，只能利用最后一个规则 第一条中的“务”和第二条中的“和”，从直观看，显然是“和”的词频在日常场景下要高，这依赖一个词频字典和的词频决定了的分词。 假设“务”作为单字词时候的频率是30，“和”作为单字词时候的频率是100，对30和100取自然对数，然后取最大值者，所以取“和”字所在的词组，即“设施_和_服务”
* 基于词频度统计的分词方法（基于统计）
  * 相邻的字同时出现的次数越多，越有可能构成一个词语，对语料中的字组频度进行统计，基于词的频度统计的分词方法是一种全切分方法
  * jieba是基于统计的分词方法，jieba分词采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合，对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法。
* 基于知识理解的分词方法
  * 该方法主要基于句法、语法分析，并结合语义分析，通过对上下文内容所提供信息的分析对词进行定界
  * 通常包括三个部分：分词子系统、句法语义子系统、总控部分
  * 在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断
  * 这类方法试图让机器具有人类的理解能力，需要使用大量的语言知识和信息
  * 由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式
  * 因此目前基于知识的分词系统还处在试验阶段。

## [jieba](https://github.com/fxsjy/jieba)

结巴中文分词

* 特点
  - 支持三种分词模式：
    + 精确模式，试图将句子最精确地切开，适合文本分析；
    + 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；
    + 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。
  - 支持繁体分词
  - 支持自定义词典
* 功能
  - 分词
  - 添加自定义词典(开发者可以指定自己自定义的词典，以便包含 jieba 词库里没有的词)
  - 关键词提取
  - 词性标注
  - 并行分词
  - ChineseAnalyzer for Whoosh 搜索引擎等
* 策略
  - 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)
  - 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合
  - 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法

```sh
pip install jieba
```
