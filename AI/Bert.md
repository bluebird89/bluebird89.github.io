# [bert](https://github.com/google-research/bert)

TensorFlow code and pre-trained models for BERT <https://arxiv.org/abs/1810.04805>

Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.

## 参考

* [TensorFlow code and pre-trained models for BERT](https://arxiv.org/abs/1810.04805)
