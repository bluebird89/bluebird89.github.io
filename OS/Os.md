# OS

### 概念

* 线程是操作系统调度的最小单位
* 进程是资源分配的最小单位
* 由于CPU是串行的,因此对于单核CPU来说,同一时刻一定是只有一个线程在占用CPU资源的。因此，Linux作为一个多任务(进程)系统，会频繁的发生进程/线程切换
* CPU上下文：在每个任务运行前，CPU都需要知道从哪里加载，从哪里运行，这些信息保存在CPU寄存器和操作系统的程序计数器里面
* 进程上下文：进程是由内核来管理和调度的，进程的切换只能发生在内核态，因此虚拟内存、栈、全局变量等用户空间的资源，以及内核堆栈、寄存器等内核空间的状态
* 线程的上下文：线程会共享父进程的虚拟内存和全局变量等资源，父进程的资源加上线上自己的私有数据
    - 如果是同一进程的线程，因为有资源共享，所以会比多进程间的切换消耗更少的资源
* 进程和线程的切换，会产生CPU上下文切换和进程/线程上下文的切换。而这些上下文切换,都是会消耗额外的CPU的资源的
* 典型PHP-FPM的CGI模式，每一个HTTP请求会经历如下，决定了在高并发上的灾难性表现
    - 都会读取框架的数百个php文件
    - 都会重新建立/释放一遍MYSQL/REIDS/MQ连接
    - 都会重新动态解释编译执行PHP文件
    - 都会在不同的php-fpm进程直接不停的切换切换再切换
* 内存模型 :主要分为语言级别的内存模型和硬件级别的内存模型。
    - 语言级别的内存模型，C/C++属于weak memory model，简单的说就是编译器在进行编译优化的时候，可以对指令进行重排，只需要保证在单线程的环境下，优化前和优化后执行结果一致即可，执行中间过程不保证跟代码的语义顺序一致。所以在多线程的环境下，如果依赖代码中间过程的执行顺序，程序就会出现问题。
    - 硬件级别的内存模型，常用的cpu，也属于弱内存模型，即cpu在执行指令的时候，为了提升执行效率，也会对某些执行进行乱序执行（按照wiki提供的资料，在x86 64环境下，只会发生读写乱序，即读操作可能会被乱序到写操作之前），如果在编程的时候不做一些措施，同样容易造成错误。
* 内存屏障 :为了解决弱内存模型造成的问题，需要一种能控制指令重排或者乱序执行程序的手段，这种技术就叫做内存屏障，程序员只需要在代码中插入特定的函数，就能控制弱内存模型带来的负面影响，当然，由于影响了乱序和重排这类的优化，对代码的执行效率有一定的影响。具体实现上，内存屏障技术分三种，
* 互斥锁 :互斥锁有两层语义，除了大家都知道的排他性（即只允许一个线程同时访问）外，还有一层内存屏障（full memory barrier）的语义，即保证临界区的操作不会被乱序到临界区外。Pthread库里面常用的mutex，conditional variable等操作都自带内存屏障这层语义。此外，使用pthread库，每次调用都需要应用程序从用户态陷入到内核态中查看当前环境，在锁冲突不是很严重的情况下，效率相对比较低。
* 自旋锁 :传统的互斥锁，只要一检测到锁被其他线程所占用了，就立刻放弃cpu时间片，把cpu留给其他线程，这就会产生一次上下文切换。当系统压力大的时候，频繁的上下文切换会导致sys值过高。自旋锁，在检测到锁不可用的时候，首先cpu忙等一小会儿，如果还是发现不可用，再放弃cpu，进行切换。互斥锁消耗cpu sys值，自旋锁消耗cpu usr值。
* 递归锁 :如果在同一个线程中，对同一个互斥锁连续加锁两次，即第一次加锁后，没有释放，继续进行对这个锁进行加锁，那么如果这个互斥锁不是递归锁，将导致死锁。可以把递归锁理解为一种特殊的互斥锁。
* 死锁 :构成死锁有四大条件，其中有一个就是加锁顺序不一致，如果能保证不同类型的锁按照某个特定的顺序加锁，就能大大降低死锁发生的概率，之所以不能完全消除，是因为同一种类型的锁依然可能发生死锁。另外，对同一个锁连续加锁两次，如果是非递归锁，也将导致死锁。

现代的cpu提供了对单一变量简单操作的原子指令，即这个变量的这些简单操作只需要一条cpu指令即可完成，这样就不用对这个操作加互斥锁了，在锁冲突不激烈的情况下，减少了用户态和内核态的切换，化悲观锁为乐观锁，从而提高了效率。此外，现在外面很火的所谓无锁编程（类似CAS操作），底层就是用了这些原子操作。gcc为了方便程序员使用这些cpu原子操作，提供了一系列__sync开头的函数，这些函数如果包含内存屏障语义，则同时禁止编译器指令重排和cpu乱序执行。

## 存储

* 页是计算机管理存储器的逻辑块，将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（许多OS中，页的大小通常为4K）
* 主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行
* Page cache:针对文件系统的,是文件的缓存,在文件层面上的数据会缓存到page cache
    - 读：通过将磁盘中的数据缓存到内存中，从而减少磁盘I/O操作提高性能
        + 磁盘访问的速度比内存慢好几个数量级（毫秒和纳秒的差距）
        + 被访问过的数据，有很大概率会被再次访问
        + 内核发起一个读请求时（例如进程发起read()请求），首先会检查请求的数据是否缓存到了page cache中，如果有，那么直接从内存中读取，不需要访问磁盘，这被称为cache命中（cache hit）
        + 如果cache中没有请求的数据，即cache未命中（cache miss），就必须从磁盘中读取数据。然后内核将读取的数据缓存到cache中，这样后续的读请求就可以命中cache了
        + page可以只缓存一个文件部分的内容，不需要把整个文件都缓存进来
    - page回写（page writeback）：在page cache中的数据更改时能够被同步到磁盘上
        + 当内核发起一个写请求时（例如进程发起write()请求），同样是直接往cache中写入，磁盘的内容不会直接更新
        + 内核会将被写入的page标记为dirty，并将其加入dirty list中。内核会周期性地将dirty list中的page写回到磁盘上，从而使磁盘上的数据和内存中缓存的数据一致
            * 用户进程调用sync() 和 fsync()系统调用
            * 空闲内存低于特定的阈值（threshold）
            * Dirty数据在内存中驻留的时间超过一个特定的阈值
    - 一个inode对应一个page cache对象，一个page cache对象包含多个物理page，其内容对应磁盘上的block
    - Cache回收：释放page，从而释放内存空间。cache回收的任务是选择合适的page释放，并且如果page是dirty的，需要将page写回到磁盘中再释放。理想的做法是释放距离下次访问时间最久的page，但是很明显不现实
        + LRU（least rencently used)算法是选择最近一次访问时间最靠前的page，即干掉最近没被光顾过的page。原始LRU算法存在的问题是，有些文件只会被访问一次，但是按照LRU的算法，即使这些文件以后再也不会被访问了，但是如果它们是刚刚被访问的，就不会被选中。
        + Two-List策略:维护了两个list，active list 和 inactive list
            + 在active list上的page被认为是hot的，不能释放。只有inactive list上的page可以被释放的。首次缓存的数据的page会被加入到inactive list中，已经在inactive list中的page如果再次被访问，就会移入active list中。两个链表都使用了伪LRU算法维护，新的page从尾部加入，移除时从头部移除，就像队列一样。如果active list中page的数量远大于inactive list，那么active list头部的页面会被移入inactive list中，从而位置两个表的平衡
        + 内核使用address_space结构来表示一个page cache https://www.linuxidc.com/Linux/2018-12/156117.htm
* Buffer cache:针对磁盘块的缓存,也就是在没有文件系统的情况下,直接对磁盘进行操作的数据会缓存到buffer cache中,例如,文件系统的元数据都会缓存到buffer cache中

```c
# address_space
struct address_space {
    struct inode            *host;              /* owning inode */
    struct radix_tree_root  page_tree;          /* radix tree of all pages */
    spinlock_t              tree_lock;          /* page_tree lock */
    unsigned int            i_mmap_writable;    /* VM_SHARED ma count */
    struct prio_tree_root   i_mmap;             /* list of all mappings */
    struct list_head        i_mmap_nonlinear;   /* VM_NONLINEAR ma list */
    spinlock_t              i_mmap_lock;        /* i_mmap lock */
    atomic_t                truncate_count;     /* truncate re count */
    unsigned long           nrpages;            /* total number of pages */
    pgoff_t                 writeback_index;    /* writeback start offset */
    struct address_space_operations *a_ops;     /* operations table */
    unsigned                long flags;         /* gfp_mask and error flags */
    struct backing_dev_info *backing_dev_info;  /* read-ahead information */
    spinlock_t              private_lock;       /* private lock */
    struct list_head        private_list;       /* private list */
    struct
```

## 网络编程模型

整个演变的过程，就是对CPU有效性能压榨的过程

* Fork进程
* 进程池/线程池
* epoll事件驱动(Nginx、node.js反人类回调)
    - 多线程+epoll的模式下,有效的压榨CPU性能
* 协程：协程需要上下文切换，但是不会产生 CPU上下文切换和进程/线程上下文的切换,因为这些切换都是在同一个线程中，即用户态中的切换，甚至可以简单的理解为，协程上下文之间的切换，就是移动了一下你程序里面的指针，CPU资源依旧属于当前线程
    - 没有IO阻塞操作,不会发生协程切换
    - 带IO阻塞操作:基于协程的php+ swoole服务比 Java + netty服务的QPS高了6倍
    - 在进程/线程切换的时候，会产生额外的CPU资源花销，特别是在用户态和内核态之间切换的时候！
* 多线程模型(IO 多路复用)
    - select/poll
        + Linux很早就提供了 select 系统调用，可以在一个进程内维持1024个连接
        + 后来加入了poll系统调用，poll做了一些改进，解决了 1024 限制的问题，可以维持任意数量的连接
        + 问题:需要循环检测连接是否有事件,如果服务器有100万个连接，在某一时间只有一个连接向服务器发送了数据，select/poll需要做循环100万次，其中只有1次是命中的，剩下的99万9999次都是无效的，白白浪费了CPU资源
    - epoll
        + Linux 2.6内核提供了新的epoll系统调用，可以维持无限数量的连接，而且无需轮询，这才真正解决了 C10K 问题
        + 各种高并发异步IO的服务器程序都是基于epoll实现的，比如Nginx、Node.js、Erlang、Golang。像 Node.js，Redis 这样单进程单线程的程序，都可以维持超过1百万TCP连接，全部归功于epoll技术
        + 基于 epoll 实现的 Reactor 模型.IO复用异步非阻塞程序使用经典的Reactor模型，它本身不处理任何数据收发。只是可以监视一个socket句柄的事件变化
            * 主进程/线程往epoll内核事件中注册socket上的读就绪亊件
            * 主进程/线程调用epoll_wait等待socket上有数据可读。
            * 当socket上有数据可读时，epoll_wait通知主进程/线程。主进程/线程则将socket可读事件放入请求队列。
            * 睡眠在请求队列上的某个工作线程被唤醒，它从socket读取数据，并处理客户请求， 然后往epoll内核事件表中注册该socket上的写就绪事件。
            * 主线程调用epoll_wait等待socket可写。
            * 当socket可写时，epoll_wait通知主进程/线程主进程/线程将socket可写事件放入请求队列。
            * 睡眠在请求队列上的某个工作线程被唤醒，它往socket上写入服务器处理客户请求
* 内核实现线程与线程之间的调度，通常一个线程是无法从头到尾占用着 cpu 的，尤其是进行 i/o 操作时，许多的系统调用都是阻塞的，此时内核保存该线程的上下文，然后挂起该线程
* 当然更多时候是由于该线程的本次运行时间耗尽，只得被挂起等待 cpu 的下一次临幸
* 关键
    - 线程的上下文切换造成的开销
        + 挂起一个进程，将这个进程在 CPU 中的状态（上下文）存储于内存中的某处
        + 在内存中检索下一个进程的上下文并将其在 CPU 的寄存器中恢复
        + 跳转到程序计数器所指向的位置（即跳转到进程被中断时的代码行），以恢复该进程
    - 线程之间对资源的竞争问题

![reactor](../_satic/eventloop.png "Optional title")

## 环境变量

* execve函数接收3个参数
    - 第一个是可执行文件的路径pathname
    - 第二个是参数的指针数组argv 指向一个NULL结尾的指针数组，每个元素都是一个指向参数字符串的指针。按照约定，argv[0]是可执行文件的名称
    - 第三个是环境变量的指针数组envp  数据结构类似。唯一的区别是，环境变量数组元素指向的字符串都是名-值对形式的，比如"PWD=/usr/droh"
    - 加载:找到pathname对应的可执行文件后，execve会调用操作系统永驻内存的loader代码，把可执行文件的代码和数据从磁盘复制到内存。然后，跳到其第一个指令或“入口点”开始执行该程序

```
int execve(const char *pathname, char *const argv[], char *const envp[]);
```

## 并发控制

* 现代操作系统以及硬件基本都支持并发程序
* 在并发程序设计中，各个进程或者线程需要对公共变量的访问加以制约
* 不同的进程或者线程需要协同工作以完成特征的任务，这就需要一套完善的同步机制
    - 在Linux内核中有相应的技术实现，包括原子操作，信号量，互斥锁，自旋锁，读写锁等。
    - InnoDB考虑到效率和监控两方面的原因，实现了一套独有的同步机制

## Socket

* 套接字（socket）是通信的基石，是支持 TCP/IP 协议的网络通信的基本操作单元
* 是网络通信过程中端点的抽象表示，包含进行网络通信必须的五种信息：连接使用的协议，本地主机的 IP 地址，本地进程的协议端口，远地主机的 IP 地址，远地进程的协议端口。
* 连接过程:建立 Socket 连接至少需要一对套接字，其中一个运行于客户端，称为 ClientSocket ，另一个运行于服务器端，称为 ServerSocket,套接字之间的连接过程可以分为三个步骤：服务器监听，客户端请求，连接确认。
    - 服务器监听：是服务器端套接字并不定位具体的客户端套接字，而是处于等待连接的状态，实时监控网络状态。
    - 客户端请求：是指由客户端的套接字提出连接请求，要连接的目标是服务器端的套接字。为此，客户端的套接字必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址和端口号，然后就向服务器端套接字提出连接请求。
    - 连接确认：是指当服务器端套接字监听到或者说接收到客户端套接字的连接请求，它就响应客户端
* 套接字的请求，建立一个新的线程，把服务器端套接字的描述发给客户端，一旦客户端确认了此描述，连接就建立好了。而服务器端套接字继续处于监听状态，继续接收其他客户端套接字的连接请求。

## 系统

* 最外层客户机Ubuntu
* 两三个小分区出来，可以装多个linux发行版
* 公用/home分区
    - 每次装linux，/直接装在其中一个小分区上，/home挂载到第三个主分区去，那里存放文档和代码数据的，这样有什么新的linux就装，文档一直在，
    - 可以装新linux时起个不重复的用户名，也在home下，完全不影响老的文档和使用环境配置
* 安装第二个Linux发行版的时候，需要注意的是，EFI分区和交换分区swap已经有可用的了，安装程序可以自动检测得到，因此不需要再关系这2个分区，只需要在磁盘剩余的空闲分区中创建这个系统本身需要的根分区/和/home分区

```sh
sudo update-grub
```

* 内核参数优化
* JVM优化
* 网络参数优化
* 事务优化
* 数据库优化
* 池化
* 内存溢出排查
* 堆外内存排查
* 网络排查
* I/O排查
* 高负载排查
* 流量录制

## 触摸板

* 选择项目：点击触摸板。
* 滚动：将两个手指放在触摸板上，然后以水平或垂直方向滑动。
* 放大或缩小：将两个手指放在触摸板上，然后收缩或拉伸。
* 显示更多命令（类似于右键单击）：使用两根手指点击触摸板，或按右下角。
* 查看所有打开的窗口：将三根手指放在触摸板上，然后朝外轻扫。
* 显示桌面：将三根手指放在触摸板上，然后朝里轻扫。
* 在打开的窗口之间切换：将三根手指放在触摸板上，然后向右或向左轻扫。
* 打开 Cortana：用三根手指点击触摸板。
* 打开操作中心：用四根手指点击触摸板。
* 切换虚拟桌面：将四根手指放在触摸板上，然后向右或向左轻扫。
* 三指
    - 上：多任务视图
    - 下：显示桌面
    - 左：切换应用
    - 右：切换应用
* 四指
    - 上：多任务视图
    - 下：显示桌面
    - 左：切换桌面
    - 右：切换桌面

## 手机系统

* Aurora

## 参考

* [cfenollosa/os-tutorial](https://github.com/cfenollosa/os-tutorial):How to create an OS from scratch
* [30dayMakeOS](git@github.com:yourtion/30dayMakeOS.git)
