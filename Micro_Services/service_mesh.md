# Service Mesh 服务网格

* Service Mesh 本质上就是微服务的动态链接器（Dynamic Linker）。基础是一个网络代理，这个网络代理会接管微服务的网络流量，然后通过一个中央控制面板进行管理，将这些流量转发到该去的地方，并在这个代理的基础之上，扩展出一系列的流量监控、限流、熔断甚至是灰度发布、分布式跟踪等能力，而不需要应用本身做出任何修改，让开发者摆脱了 SDK 之苦，也避免了由于 SDK 使用不当造成的一系列问题。同时，这个代理工作是在网络层，一般情况下也不会成为性能瓶颈。
* A service mesh is a dedicated infrastructure layer for handling
service-to-service communication. It’s responsible for the
reliable delivery of requests through the complex topology of
services that comprise a modern, cloud native application. In
practice, the service mesh is typically implemented as an array
of lightweight network proxies that are deployed alongside
application code, without the application needing to be
aware
* 服务网格（ Service Mesh ）是解决微服务之间的网络问题和可观测性问题的(事实)标准，并且正在走向标准化
    - 应用程序间通讯中间层
    - 轻量级网络代理
    - 应用程序无感知
    - 解耦应用程序的重试/超时、监控、追踪和服务发现
* control plane
* 功能
    - 流量控制: (路由,流量转移,超时重试, 熔断,故障注入,流量镜像)
    - 策略 (限流、黑白名单)
    - 网络安全 (授权与身份认证)
    - 可观察性 (指标、日志、追踪)
* 和 Kubernetes 关系
* 和API 网关
* 标准
    - UDPA
    - SMI
* 产品
    - linkerd
    - envoy:数据平面
    - lstio：增加控制平面，收购enovy
    - AWS：App Mesh

## 历程

* 原始的主机之间直接使用网线相连
* 网络层的出现
* 出现网络层（4层协议）控制的需求
* 控制逻辑下移到网络
* 出现新的应用层（7层协议）需求（服务发现、熔断、超时重试等）
* 封装成三方库（服务发现：Dubbo/HSF）
    - 原本在进程中互相调用那么简单的事情，都要变成一次在 7 层网络上的远程调用
    - 原本公共工具类做的事情，现在需要写成二方库 SDK 等，在每一个进程中使用，版本迭代成为了灾难
    - 原本是内部透明调用的不需要做任何防护，分离后却要额外增加安全防护和隔离的工作
    - 不再是代码即文档，需要维护大量的 API 定义和版本管理
* Sidecar模式：通过给应用服务加装一个“边车”来达到控制和逻辑的分离的目的
    - 服务网格技术中常用的(其中)一种设计架构，在 Kubernates 中，不同的容器允许被运行在同一个 Pod 中（即多个进程运行在同一个 cgroup 下），这在很大程度上给 Sidecar 模式提供了良好的土壤
    - 微服务之间的调用在架构图中是横向的，被称为东西流量。服务暴露到外部被公网可见的外部调用，被称为南北流量
    - Consumer 与 Provider 就是微服务互相调用的一种解决方案
    - Dubbo 架构：解决的正是东西流量的问题
        + 基于 SPI 机制以一种较为隔离的方式侵入到运行时的代码中
        + 只能限定 Java 这样被官方支持的语言来开发服务应用
    - 问题：流量管理（服务发现、负载均衡、路由、限流、熔断、容错等）、可观测性（监控、日志聚合、计量、跟踪）、安全（认证、授权），再甚至更高级的动态配置、故障注入、镜像流量等
    - Sidecar 的模式更为巧妙并更进一步。通过容器机制，在进程上是隔离的，基于 L7 代理进行通讯，允许微服务是由任何语言进行开发的
* 所有使用中间件的服务组成了一个大的服务网格。服务网格基于 Kubernates 这样的容器技术，将东西流量的问题解决得更加透明无感

* 通讯层的实现方式，有以下选择：
    - 用库的形式在微服务应用程序中导入使用:每个微服务应用程序包中都有实现Service Mesh功能的库。像Hystrix和Ribbon就是用库的方法
        + 调用方式是进程内的，没有安全隔离的包袱
        + 随着编程语言的发展，新的语言为特定的场景而生，而SDK库的方式限制了使用方必须用支持列表中的语言,用不同语言去重复实现多次，挑战在于实现的复杂性和一遍又一遍去实现同样概念的工作量
    - 节点代理或守护程序的形式为特定节点/计算机上的所有容器提供服务
        + 每个节点上都运行一个单独的代理（通常是用户进程），为异构的服务提供负载
        + 代理接口的调用路由规则，转发到特定的机器
        + 使用一个特定的服务专门代理微服务中的请求，是一个中间人的角色。但这个代理人的安全性要求非常高，因为需要处理来自不同微服务的请求，并鉴别它们各自的身份
        + 由于每个节点上都需要一个节点代理，因此需要与基础架构进行一些协作，如果没有协作的话此模型就无法工作
        + 强调工作资源共享，如果节点代理用一些内存来缓存微服务的数据，那么服务就可能会在几秒钟内转向并使用该缓存区提供的数据
    - 用Sidecar容器的形式运行，和应用容器一同运行:透明地劫持所有应用容器的出入流量：介于 SDK 库和节点代理中间的一种形式，相当于是给每个微服务都配上一个自己独有的代理，每个微服务自己的 Sidecar 就代表了自己特定的身份，有利于调用的安全审计

## Sidecar 模式

* 允许为应用程序添加许多功能，而无需额外第三方组件的配置和代码的修改
* 概念
    - Sidecar 模式：容器应用模式之一，Service Mesh 架构的一种实现方式
    - Init 容器：Pod 中的一种专用的容器，在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本
        + 使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图
    - iptables：流量劫持是通过 iptables 转发实现的，含 5 张表：
        + raw 用于配置数据包，raw 中的数据包不会被系统跟踪
        + filter 是用于存放所有与防火墙相关操作的默认表
        + nat 用于 网络地址转换（例如：端口转发）
        + mangle 用于对特定数据包的修改（参考损坏数据包）
        + security 用于强制访问控制 网络规则
* 步骤：
    - Kubernetes 需要了解待注入的 sidecar 所连接的 Istio 集群及其配置
    - Kubernetes 需要了解待注入的 sidecar 容器本身的配置，如镜像地址、启动参数等
    - Kubernetes 根据 sidecar 注入模板和以上配置填充 sidecar 的配置参数，将以上配置注入到应用容器的一侧
* Sidecar 模式解决微服务之间的网络通讯（远程调用）
* 与网关模式有类似之处，但是其粒度更细。其为每个服务都配备一个“边车”，这个“边车“可以理解为一个 agent ，这个服务所有的通信都是通过这个 agent 来完成的，这个 agent 同服务一起创建，一起销毁。像服务注册、服务发现、监控、流量控制、日志记录、服务限流和服务熔断等功能完全可以做成标准化的组件和模块，不需要在单独实现其功能来消耗业务开发的精力和时间来开发和调试这些功能，这样可以开发出真正高内聚低耦合的软件
* 优势
    - 通过抽象出与功能相关的共同基础设施到一个不同层降低了微服务代码的复杂度
    - 因为不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度
    - 降低应用程序代码和底层平台的耦合度
* Istio CNI 插件设计目标是消除这个 privileged 权限的 init container，换成利用 Kubernetes CNI 机制来实现相同功能的替代方案

## 原理

* 服务网格中分为控制平面和数据平面，当前流行的两款开源的服务网格 Istio 和 Linkerd 实际上都是这种架构
* Istio 的划分更清晰，而且部署更零散，很多组件都被拆分，控制平面中包括 Mixer、Pilot、Citadel，数据平面默认是用 Envoy
* Linkerd 中只分为 Linkerd 做数据平面，namerd 作为控制平面
* 控制平面
    - 不直接解析数据包
    - 与数据平面中的代理通信，下发策略和配置
    - 负责网络行为的可视化
    - 通常提供 API 或者命令行工具可用于配置版本化管理，便于持续集成和部署
* 数据平面
    - 通常是按照无状态目标设计的，但实际上为了提高流量转发性能，需要缓存一些数据，因此无状态也是有争议的
    - 直接处理入站和出站数据包，转发、路由、健康检查、负载均衡、认证、鉴权、产生监控数据等
    - 对应用来说透明，即可以做到无感知部署

## 实现服务网格的方法

* 改进微服务的消息处理机制。服务网格确保你能监控到整个架构层，不仅可以跟踪到网络中的服务器地址，还可以跟踪到传达服务器地址信息的消息。例如，你可能想要跟踪“失败”消息，但这些消息在传统云架构中通常会丢失。服务网格的好处是既可以确保消息的传递，又会在消息未到达目的地时返回错误信息。
* 利用与传统应用程序相同的运维方式。对于企业级网络来说，可定制性和灵活性是最重要的。服务网格是为适应现代分布式应用程序而设计的。但是底层的技术如入口控制器，负载均衡器，以及代理都和传统单体应用的数据层面的技术相同。在实现服务网格的过程中，组织可以利用到与运营现代、基于软件的应用程序交付基础设施相同的技术与技能。
* 灵活使用多种云服务。服务网格解决了现代应用的云网络问题。支撑起服务网格的数据平面和控制平面的技术独立于任何特定架构，因此它们可以在无论是裸机，容器还是虚拟机的公有或私有的架构上运行。这种灵活特性甚至允许服务网格处理未来的应用程序架构，从而发挥其规模化、全球复制以及深层性能调节等优势。您的服务网格将成为运作模式化云架构场景下，一切潜在优势的实现保障。
* 提高对微服务的可见性。分布式系统的指标对于我们而言就像是一个黑盒子，而网格服务为我们提供了一种更深入观察分布式系统的指标的途径。它会随时间收集性能指标，为团队提供服务可用性的长期指标。这为操作员提供了一种观察服务可靠性和性能的方式，使他们能够逐步优化系统。
* 更高效的运维以及更有效的执行SLA（服务等级协议）。服务网格提供的追踪功能对调试和故障排除至关重要，与此同时，它也确保服务执行了服务等级协议（SLA）。服务网格执行了很多任务，包括执行策略以及追踪查看这些策略是否被满足。它为管理者提供了一个可以在网络层实施云应用管理和策略的场所。
* 简化微服务实现。服务网格的另一大优点是可以轻松部署它们。过去的解决方案要求开发人员将服务内功能编码到每个微服务中。这需要重写应用程序并在不同的编程语言中维护各种库。而服务网格帮开发人员抽象了这些事务。开发人员可以简单地调用必要的消息传递和服务发现功能就可以轻松的部署它们，而微服务的源码只用包含业务逻辑相关的代码。
* 加快新服务的上线时间。过去的库解决方案，如Finagle，Hystrix和Stubby，需要开发人员长时间的介入并且迫使开发人员将冗余功能编码到每一个服务中。另一个更简单的方法是在每个微服务中放置一个sidecar代理并将它们连接在一起，这正是服务网格所擅长的，因此未来将会有更多的云应用选择服务网格架构。简而言之，服务网格保证了开发者的生产力，使他们能够更快地将更多的服务推向市场。
* 保障服务间的通信安全。服务之间通信有可能跨云，跨数据中心，或者跨大陆，而服务网格保障了这些通信的安全，它封装了所有的通信，并且在控制器层面协调这些通信，通过管道内加密，联系人策略和服务权限解决了安全问题。

## 工具

* [rootsongjc / kubernetes-vagrant-centos-cluster](https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster):Setting up a distributed Kubernetes cluster along with Istio service mesh locally with Vagrant and VirtualBox, only PoC or Demo use. https://jimmysong.io

## 参考

* [ServicemeshCN/awesome-servicemesh](https://github.com/ServicemeshCN/awesome-servicemesh):A curated list for awesome service mesh architectures https://servicemesh.gitbooks.io/aweso…
* [ geektime-geekbang / geektime-servicemesh ](https://github.com/geektime-geekbang/geektime-servicemesh)
* Pattern:service Mesh
